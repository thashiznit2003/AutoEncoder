version: "3.8"

services:
  autoencoder:
    build: .
    image: linux-video-encoder:latest
    container_name: linux-video-encoder
    restart: unless-stopped
    ports:
      - "5959:5959"
    environment:
      - TZ=Etc/UTC
      # Enable NVIDIA GPU (requires NVIDIA Container Toolkit on the Docker host)
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,video,utility
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      # Bind config and media paths (host-relative to repo) into the container.
      - ./config.json:/linux-video-encoder/config.json
      - ./USB:/mnt/usb:rw
      - ./DVD:/mnt/dvd:rw
      - ./Bluray:/mnt/bluray:rw
      - ./File:/mnt/input:rw
      - ./Output:/mnt/output:rw
      - ./Ripped:/mnt/ripped:rw
    privileged: true
    devices:
      # Map your optical drive and (optionally) generic SCSI device into the container.
      - /dev/sr0:/dev/sr0
      - /dev/sg0:/dev/sg0
      # Pass USB bus for auto-mounting flash drives (requires host device access).
      - /dev/bus/usb:/dev/bus/usb
      # Uncomment if you want Intel QuickSync or another GPU exposed to ffmpeg/HandBrakeCLI.
      # - /dev/dri:/dev/dri
      # Map NVIDIA device nodes if you prefer explicit devices (NVIDIA Container Toolkit handles this automatically when installed).
      # - /dev/nvidia0:/dev/nvidia0
      # - /dev/nvidiactl:/dev/nvidiactl
      # - /dev/nvidia-uvm:/dev/nvidia-uvm
    # Uncomment if you need the container to mount/unmount devices itself (generally not needed if you bind mount).
    # privileged: true
